{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW 13",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQEkfEwBHIcb"
      },
      "source": [
        "##**Metrics**\n",
        "\n",
        "####**Explain in words what accuracy, precision, and recall are.  Describe a situation when you would prefer one to another and where the shortcomings to each lays.**\n",
        "\n",
        "Accuracy is the most intuitive performance measure. It is a ratio of correctly predicted observation to the total observations. Accuracy is a great measure when you have symmetric datasets where values of false positive and false negatives are almost same.\n",
        "TP+TN/TP+FP+FN+TN\n",
        "#####Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.TP/TP+FP\n",
        "Recall (or sensitivity) is the ratio of correctly predicted positive observations to the all observations.\n",
        "TP/TP+FN\n",
        "#####F1 Score is the weighted average of Precision and Recall. This score takes both false positives and false negatives into account. F1 is usually more useful than accuracy, especially if you have an uneven class distribution. If the cost of false positives and false negatives are very different, itâ€™s better to look at both Precision and Recall rather than Accuracy. 2(Recall Precision) / (Recall + Precision)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDaAyMVZUv0o"
      },
      "source": [
        "####**What is a confusion matrix?**\n",
        "\n",
        "A confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). \n",
        "#####Here we create our confusion matrix and study the different, relevant points. We have four sections in the confusion matrix:\n",
        "\n",
        "True Positives (TP): True positives are the cases when the actual class of the data point was 1(True) and the predicted is also 1(True)\n",
        "#####True Negatives (TN): True negatives are the cases when the actual class of the data point was 0(False) and the predicted is also 0(False)\n",
        "#####False Positives (FP): False positives are the cases when the actual class of the data point was 0(False) and the predicted is 1(True). False is because the model has predicted incorrectly and positive because the class predicted was a positive one.\n",
        "False Negatives (FN): False negatives are the cases when the actual class of the data point was 1(True) and the predicted is 0(False). False is because the model has predicted incorrectly and negative because the class predicted was a negative one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG-yNWyKw0mZ"
      },
      "source": [
        "####**Write the python code for accuracy, precision, recall, and F1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsscRHQSxAl4",
        "outputId": "2363f3ba-1d5c-49aa-855a-45e63aabbef7"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = [0, 2, 1, 3]\n",
        "y_true = [0, 1, 2, 3]\n",
        "\n",
        "accuracy_score(y_true, y_pred)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va3r1f3nxn7U",
        "outputId": "0ae53094-27ee-4d46-d13c-ae6275d87290"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "y_true = [0, 1, 2, 0, 1, 2]\n",
        "y_pred = [0, 2, 1, 0, 0, 1]\n",
        "precision_score(y_true, y_pred, average='macro')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2222222222222222"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYqbP4ZGyJNC",
        "outputId": "855f5121-3829-47dd-cc0e-bdec19f317e1"
      },
      "source": [
        "from sklearn.metrics import recall_score\n",
        "y_true = [0, 1, 2, 0, 1, 2]\n",
        "y_pred = [0, 2, 1, 0, 0, 1]\n",
        "recall_score(y_true, y_pred, average='macro')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XBVVjmlywHm",
        "outputId": "632654d4-759a-4b21-c7e3-fc94d80ef665"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "y_true = [0, 1, 2, 0, 1, 2]\n",
        "y_pred = [0, 2, 1, 0, 0, 1]\n",
        "f1_score(y_true, y_pred, average='macro')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK5bVwMLzRyN"
      },
      "source": [
        "####**Give your own example of a type 1 and type 2 error**\n",
        "\n",
        "**Coronavirus test**\n",
        "#####A person decided to get tested for COVID-19 based on mild symptoms. There are two errors that could potentially occur:\n",
        "#####Type 1 error ( false positive ): The test result says that a person has a coronavirus, but actually, they don't.\n",
        "#####Type 2 error ( false negative ): The test result says that a person doesn't have a coronavirus, but they do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgCANlAs2eN1"
      },
      "source": [
        "####**Why do we use train.test,split() function from Python when analyzing data?  What is the point of splitting data?**\n",
        "\n",
        "#####When the dataset is split into train and test sets, there will be enough data in training dataset for the model to learn an effective mapping of inputs to outputs. Most data is used for training and the small portion is used for testing. Analysing random samples of the data help to wnsure if the testing and training sets are similar. It also prevents the data from becoming overfitting or underfitting and to accurately evaluate your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMCnF4VB20ov"
      },
      "source": [
        "####**What is the bias vs. variance tradeoff?**\n",
        "\n",
        "#####Bias is the simplifying assumptions made by the model to make the target function easier to approximate.\n",
        "#####Variance is the amount that the estimate of the target function will change given different training data.\n",
        "#####Trade-off is tension between the error introduced by the bias and the variance."
      ]
    }
  ]
}